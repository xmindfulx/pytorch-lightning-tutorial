{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064f730e",
   "metadata": {},
   "source": [
    "## <ins> Configuration Parameters </ins>\n",
    "Every experiment needs some control parameters, so here is a basic way to initialize them and prepare them for development. **The most important parameters for running this tutorial are the following:**\n",
    "\n",
    "\n",
    "- `path_results`: path to results folder\n",
    "\n",
    "\n",
    "- `path_datasets` : path to dataset folder\n",
    "\n",
    "Note this configuration setup also supports Nvidia GPU acceleration. However, make sure that you have an environment that supports GPU driver and CUDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: MINDFUL\n",
    "# Purpose: Configuration ( Linear Regression )\n",
    "\n",
    "#--------------------------------\n",
    "# Parameters: All Paths (I / O)\n",
    "#--------------------------------\n",
    "\n",
    "path_results = \"../../results/mlp/\"\n",
    "path_dataset = \"../../data/classification/data.csv\"\n",
    "\n",
    "#-------------------------------\n",
    "# Parameters: Training Model\n",
    "#-------------------------------\n",
    "\n",
    "# Config: Validation Rate\n",
    "\n",
    "valid_rate = 1\n",
    "\n",
    "# Config: Randomization\n",
    "\n",
    "seed = 123 \n",
    "\n",
    "# Config: CPU \n",
    "\n",
    "num_workers = 1\n",
    "\n",
    "# Config: GPU\n",
    "\n",
    "use_gpu = 0\n",
    "gpu_list = [0, 1]\n",
    "\n",
    "# Config: Gradient Descent\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 100 \n",
    "learning_rate = 0.01\n",
    "\n",
    "# Config: Logger \n",
    "# - 0 : Tensorboard\n",
    "# - 1 : Custom Logger\n",
    "\n",
    "logger_choice = 0\n",
    "\n",
    "# Create: Parameter Container \n",
    "\n",
    "params = { \"path_results\": path_results, \"path_dataset\": path_dataset,\n",
    "           \"valid_rate\": valid_rate, \"seed\": seed, \"num_workers\": num_workers, \"use_gpu\": use_gpu, \n",
    "           \"gpu_list\": gpu_list, \"batch_size\": batch_size, \"num_epochs\": num_epochs, \"learning_rate\": learning_rate }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278515d8",
   "metadata": {},
   "source": [
    "## <ins> Verbose Warnings </ins>\n",
    "\n",
    "Pytorch lightning is notorious of its warnings. Some can be helpful during the debugging process. Others can be things like suggestions to improve performance. B/c of this, going to share how to filter them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19437ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# Remove: Irrelevant Warnings\n",
    "#--------------------------------\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a06d41",
   "metadata": {},
   "source": [
    "## <ins> Python Libraries </ins> \n",
    "\n",
    "Big strength of python is its large library support. Lets import some and discuss their importance. \n",
    "\n",
    "**Standard Libraries**\n",
    "\n",
    "- `warnings`: Controls i/o with respect to warnings\n",
    "\n",
    "\n",
    "- `numpy`: Linear algebra, data representation (e.g., matrices, vectors), and more\n",
    "\n",
    "\n",
    "- `matplotlib`: Visualizations / Plots\n",
    "\n",
    "\n",
    "- `torch`: the original pytorch, a modern library for nerual network applications. \n",
    "\n",
    "\n",
    "-`pytorch_lightning`: Pytorch but with more tools that support organization and simplification. There are a lot of libraries to important, and in a later tutorial we will explore their importance. However, for now, treat as magic!\n",
    "\n",
    "\n",
    "**Custom Libraries** ( I made these - not discussed in this tutorial, but feel free to take a look! )\n",
    "\n",
    "\n",
    "- Loader: Convert dataset to pytorch format\n",
    "\n",
    "\n",
    "- Logger: Experiment logging tool for results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9fb3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# Import: Basic Python Libraries\n",
    "#--------------------------------\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#--------------------------------\n",
    "# Import: Pytorch Libraries\n",
    "#--------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Optional\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from pytorch_lightning.loggers import LightningLoggerBase\n",
    "from pytorch_lightning.loggers.base import rank_zero_experiment\n",
    "from pytorch_lightning import LightningModule, LightningDataModule\n",
    "\n",
    "#--------------------------------\n",
    "# Import: Custom Python Libraries\n",
    "#--------------------------------\n",
    "\n",
    "from custom_logger import Logger\n",
    "from loader import Dataset as Pytorch_Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-gardening",
   "metadata": {},
   "source": [
    "## <ins> Dataset Loader </ins>\n",
    "\n",
    "First thing to do is write a loader for our dataset. We are using .csv files in the following format:\n",
    "\n",
    "\n",
    "- **rows** : observation samples of dataset\n",
    "\n",
    "\n",
    "- **columns** : features of observation samples **(last column is supervised label of observation sample)**.\n",
    "\n",
    "**Code Breakdown:**\n",
    "\n",
    "\n",
    "- `Dataset`: basic way to represent a dataset ( observed samples, labels )\n",
    "\n",
    "\n",
    "- `load_data()`: load a .csv file with the aforementioned format. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d5d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# Initialize: Custom Dataset \n",
    "#--------------------------------\n",
    "\n",
    "class Dataset:\n",
    "\n",
    "    def __init__(self, samples, labels):\n",
    "\n",
    "        self.labels = labels\n",
    "        self.samples = samples\n",
    "\n",
    "#--------------------------------\n",
    "# Load: Training Dataset (.CSV)\n",
    "#--------------------------------\n",
    "\n",
    "def load_data(path):\n",
    "\n",
    "    data_file = open(path, \"r\")\n",
    "\n",
    "    data = []\n",
    "    for line in data_file:\n",
    "        data.append([ float(ele.strip(\"\\n\")) for ele in line.split(\",\") ])\n",
    "\n",
    "    data = np.asarray(data)\n",
    "\n",
    "    samples, labels = data[:, :-1], data[:, -1]\n",
    "\n",
    "    return Dataset(samples, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-canon",
   "metadata": {},
   "source": [
    "## <ins> Pytorch Lightning Dataset </ins>\n",
    "\n",
    "Pytorch lightning shines when using a dataloader (i.e., a specific format for a dataset). The dataloader bestows benefits like implicit shuffling, mini-batch sample support, improved cpu parallelization. \n",
    "\n",
    "Because of this lets make a **Pytorch Lightning Data Module (PLDM)** that takes a dataset and converts it into a pytorch dataloader. This is requires two steps:\n",
    "\n",
    "- Convert basic dataset to pytorch dataset. Specifically, I provide code for this ( please take a look if you're interested. Not too difficult ), but will not go into detail for this tutorial unless asked!\n",
    "\n",
    "\n",
    "- Convert pytorch dataset to pytorch dataloader. \n",
    "\n",
    "**Code Breakdown:**\n",
    "\n",
    "\n",
    "- `PLDM`: our pytorch lightning class that converts a dataset into pytorch format. \n",
    "\n",
    "\n",
    "- `setup()`: method that converts dataset into pytorch dataset. Note for this method, we don't have a validation dataset so we are instead doing re-substitution (i.e., evaluating on the training dataset). \n",
    "\n",
    "\n",
    "- `train_dataloader()`, `val_dataloader()`: methods that converts pytorch datasets into pytorch dataloaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# Create: Lightning Data Module\n",
    "#--------------------------------\n",
    "\n",
    "class PLDM(LightningDataModule):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super().__init__() \n",
    "                           \n",
    "        # Load: Dataset Parameters\n",
    "                           \n",
    "        self.data = params[\"train\"]\n",
    "                           \n",
    "        # Load: Processing Parameters\n",
    "\n",
    "        self.batch = params[\"batch_size\"]\n",
    "        self.workers = params[\"num_workers\"]\n",
    "        \n",
    "    #----------------------------\n",
    "    # Create: Training Datasets \n",
    "    #----------------------------\n",
    "                           \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "\n",
    "        # Create: Pytorch Datasets\n",
    "\n",
    "        self.train = Pytorch_Dataset(self.data)\n",
    "        self.valid = Pytorch_Dataset(self.data)\n",
    "\n",
    "    #----------------------------\n",
    "    # Create: Training DataLoader\n",
    "    #----------------------------\n",
    "\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        return DataLoader( self.train, batch_size = self.batch,\n",
    "                           num_workers = self.workers, shuffle = 1, persistent_workers = 1 )\n",
    "\n",
    "    #----------------------------\n",
    "    # Create: Validation Loader\n",
    "    #----------------------------\n",
    "\n",
    "    def val_dataloader(self):\n",
    "\n",
    "        return DataLoader( self.valid, batch_size = self.batch,\n",
    "                           num_workers = self.workers, persistent_workers = 1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-recovery",
   "metadata": {},
   "source": [
    "## <ins> Pytorch Lightning Model </ins>\n",
    "\n",
    "Pytorch is a neural network framework. So any models we design using it follow a very similar structure:\n",
    "\n",
    "- **Architecture** : Topology of the network in layers where each layer can have a set of optimizable parameters\n",
    "\n",
    "\n",
    "- **Objective Function** : Function representing the error of the goal the network is trying to perform. \n",
    "\n",
    "\n",
    "- **Optimization Function** : Function representing how the model updates its parameters (e.g, flavors of gradient descent)\n",
    "\n",
    "\n",
    "**Code Breakdown:**\n",
    "\n",
    "\n",
    "- `Linear_Regression`: our pytorch class for implementing the linear regression algorithm\n",
    "\n",
    "\n",
    "- `objective()`: method that defines objective function of the model. Specifically for this example we use Mean Squared Error (MSE). \n",
    "\n",
    "\n",
    "- `configure_optimizers()`: method that defines optimization paradigm for learning. Specifcally for this example we use a variation of gradient descent called Adaptive Momentum Estimation (ADAM). \n",
    "\n",
    "\n",
    "- `forward()`: method that defines calculations leading to model prediciton. \n",
    "\n",
    "\n",
    "- `training_step()`: method that defines how the training dataset interacts with the model. The training step includes: **(Forward pass)** Data is fed to the model and the model undergoes calculations leading to make a prediciton. **(Error Assessment)** the model prediciton is compared to a truth label and error is calculated to guide learning. **(Backwards pass)** the models learnable paremters are updated using the objective loss. It is important to note that this section of code explictly shows the forward pass and objective loss calculations. However, the backwards pass is handed through auto-differentiation implicitly. \n",
    "\n",
    "\n",
    "- `training_epoch_end()`: method that defines any addtional logging / algorithms / assessments that you want to run after a complete training cycle (epoch)\n",
    "\n",
    "\n",
    "- `validation_step()`: method that defines how the validation dataset intereacts with the model. \n",
    "\n",
    "\n",
    "- `validation_epoch_end()`: logging / algorithms / assessments that you want to run after a complete vdalidation cycle. I personally find this useful for recording performance metrics (e.g., precision, accuracy, recall) or other visualizations (e.g., learned feature embeddings, learned weights, generated imagery). \n",
    "\n",
    "\n",
    "- `calculate_performance()`: method that creates a confusion matrix with respect to truth labels and model predictions\n",
    "\n",
    "\n",
    "\n",
    "- `calculate_statistics()`: function that calculates precision, recall, and accuracy performance metrics.\n",
    "\n",
    "\n",
    "\n",
    "- `prepare_world()`: method that creates and evaluates 2D feature space \n",
    "\n",
    "- `log_features()`: method that illustrates learned decision boundary and saves via tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c6184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# Import: Basic Python Libraries\n",
    "#--------------------------------\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn                                                                      \n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "#--------------------------------\n",
    "# Initialize: Lightining Model\n",
    "#--------------------------------\n",
    "\n",
    "class MLP(LightningModule):\n",
    "\n",
    "    def __init__(self, params):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Load: Model Parameters\n",
    "        \n",
    "        self.max_epochs = params[\"num_epochs\"]\n",
    "        self.learning_rate = params[\"learning_rate\"]\n",
    "\n",
    "        # Initialize: MLP Model \n",
    "\n",
    "        self.evaluate = nn.Sequential( nn.Linear(2, 5),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(5, 10),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(10, 2) )\n",
    "\n",
    "    #----------------------------\n",
    "    # Create: Objective Function\n",
    "    #----------------------------\n",
    "\n",
    "    def objective(self, preds, labels):\n",
    "    \n",
    "        # Format: Labels\n",
    "\n",
    "        labels = labels.long()\n",
    "\n",
    "        # Objective: Mean Squared Error\n",
    "\n",
    "        cost = nn.CrossEntropyLoss() \n",
    "\n",
    "        loss = cost(preds, labels) \n",
    "\n",
    "        # Logging: Loss\n",
    "\n",
    "        self.log(\"loss\", loss, on_step = True, on_epoch = True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    #----------------------------\n",
    "    # Create: Optimizer Function\n",
    "    #----------------------------\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    #----------------------------\n",
    "    # Create: Model Forward Pass\n",
    "    #----------------------------\n",
    "\n",
    "    def forward(self, samples):\n",
    "\n",
    "        return self.evaluate(samples)\n",
    "\n",
    "    #----------------------------\n",
    "    # Create: Train Cycle (Epoch)\n",
    "    #----------------------------\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # Load: Data Batch\n",
    "\n",
    "        samples, labels = batch\n",
    "\n",
    "        preds = self(samples)\n",
    "\n",
    "        # Calculate: Training Loss\n",
    "\n",
    "        loss = self.objective(preds, labels)\n",
    "       \n",
    "        return loss\n",
    "\n",
    "    #----------------------------\n",
    "    # Run: Post Training Script\n",
    "    #----------------------------\n",
    "\n",
    "    def training_epoch_end(self, train_step_outputs): \n",
    "\n",
    "        # Update: Training Plots\n",
    "\n",
    "        if(logger_choice == 1):\n",
    "            \n",
    "            if(self.current_epoch > 0):\n",
    "\n",
    "                logger = self.logger.experiment\n",
    "\n",
    "                logger.log_training_loss(self.current_epoch)\n",
    "\n",
    "                # Finalize: Learned Features & Metrics ( Video )\n",
    "\n",
    "                if(self.current_epoch == self.max_epochs - 1):\n",
    "\n",
    "                    logger.finalize_results()\n",
    "\n",
    "    #----------------------------\n",
    "    # Create: Validation Cycle \n",
    "    #----------------------------\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        samples, labels = batch\n",
    "\n",
    "        preds = self(samples)\n",
    "    \n",
    "        return samples, labels, preds\n",
    "\n",
    "    #----------------------------\n",
    "    # Run: Post Validation Script\n",
    "    #----------------------------\n",
    "\n",
    "    def validation_epoch_end(self, val_step_outputs): \n",
    "\n",
    "        # Organize: Validation Outputs\n",
    " \n",
    "        all_samples, all_labels, all_preds = [], [], []\n",
    "    \n",
    "        for group in val_step_outputs:\n",
    "\n",
    "            samples, labels, preds = group\n",
    "\n",
    "            all_labels.append( labels )\n",
    "            all_samples.append( samples )\n",
    "            all_preds.append( preds.detach() )\n",
    "\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_samples = torch.cat(all_samples)\n",
    "\n",
    "        # Calculate: Performance\n",
    "\n",
    "        results = self.calculate_performance(all_preds, all_labels)\n",
    "\n",
    "        # Logger: Performance\n",
    "\n",
    "        self.log(\"recall\", results[\"model_recall\"], on_epoch = True)\n",
    "        self.log(\"accuracy\", results[\"model_accuracy\"], on_epoch = True)\n",
    "        self.log(\"precision\", results[\"model_precision\"], on_epoch = True)\n",
    "        \n",
    "        preds, feature_space = self.prepare_world(all_samples)\n",
    "            \n",
    "        if(self.current_epoch > 0):\n",
    "            \n",
    "            if(logger_choice == 0):\n",
    "                \n",
    "                # Logger: Feature Visualizations  \n",
    "                \n",
    "                self.log_features(all_samples, all_labels, feature_space, preds, self.current_epoch)\n",
    "                \n",
    "            else:\n",
    "                    \n",
    "                logger = self.logger.experiment\n",
    "\n",
    "                # Logger: Validation Plots\n",
    "\n",
    "                logger.log_valid_results(self.current_epoch)\n",
    "\n",
    "                # Logger: Feature Visualizations  \n",
    "\n",
    "                logger.log_features(all_samples, all_labels, feature_space, preds, self.current_epoch)\n",
    "\n",
    "    #----------------------------\n",
    "    # Run: Validation Metrics\n",
    "    #----------------------------\n",
    "\n",
    "    def calculate_performance(self, all_preds, all_labels):\n",
    "\n",
    "        unique_labels = np.unique(all_labels.numpy())\n",
    "\n",
    "        confusion_matrix = np.zeros( [len(unique_labels), len(unique_labels)] )\n",
    "\n",
    "        for pred, label in zip(all_preds, all_labels):\n",
    "\n",
    "            label = int(label)\n",
    "\n",
    "            prediction = np.argmax(pred.numpy())\n",
    "            \n",
    "            confusion_matrix[label][prediction] += 1                \n",
    "               \n",
    "        confusion_matrix = confusion_matrix.astype(int)        \n",
    "     \n",
    "        return calculate_statistics(confusion_matrix)\n",
    "\n",
    "    #----------------------------\n",
    "    # Generation: Feature Space\n",
    "    #----------------------------\n",
    "    \n",
    "    def prepare_world(self, samples, offset = 0.5, precision = 0.05):\n",
    "\n",
    "        # Gather: Mins, Maxes Dataset ( Adjust Offset )\n",
    "\n",
    "        y_min = torch.min(samples[:, 0]) - offset\n",
    "        y_max = torch.max(samples[:, 0]) + offset\n",
    "        x_min = torch.min(samples[:, 1]) - offset\n",
    "        x_max = torch.max(samples[:, 1]) + offset\n",
    "\n",
    "        # Create: 2D Feature Space \n",
    "    \n",
    "        y_vals = torch.arange(y_min, y_max, precision)\n",
    "        x_vals = torch.arange(x_min, x_max, precision)\n",
    "\n",
    "        all_points = [ [y, x] for y in y_vals for x in x_vals ]\n",
    "\n",
    "        all_points = torch.tensor(all_points)\n",
    "\n",
    "        # Evaluate: 2D Feature Space ( MLP )\n",
    "\n",
    "        predictions = []\n",
    "        for sample in all_points:\n",
    "            \n",
    "            sample = torch.unsqueeze(sample, dim = 0)\n",
    "            predictions.append( torch.argmax(self(sample).detach()) )\n",
    "            \n",
    "        return torch.tensor(predictions), all_points\n",
    "\n",
    "    #----------------------------\n",
    "    # Logging: Feature Embeddings\n",
    "    #----------------------------\n",
    "\n",
    "    def log_features(self, samples, labels, feature_space, preds, epoch, z = 4, f_s = 20, p_s = (15, 11)):\n",
    "\n",
    "        # Assign: Figure Name\n",
    "\n",
    "        name = str(epoch).zfill(z) + \".png\"\n",
    "\n",
    "        # Format: Plot\n",
    "\n",
    "        plt.style.use(\"seaborn\")\n",
    "\n",
    "        # Assign: Colors\n",
    "\n",
    "        face_colors = [ \"blue\" if(ele == 0) else \"red\" for ele in labels ]\n",
    "        back_colors = [ \"darkblue\" if(ele == 0) else \"darkred\" for ele in preds ]\n",
    "\n",
    "        # Plot: Dataset & Feature Space\n",
    "\n",
    "        fig, ax = plt.subplots(figsize = p_s)\n",
    "\n",
    "        ax.scatter( feature_space[:, 0], feature_space[:, 1], c = back_colors )\n",
    "        ax.scatter( samples[:, 0], samples[:, 1], s = 200, \n",
    "                    linewidths = 3, edgecolor = \"black\", c = face_colors )\n",
    "\n",
    "        ax.set_xlabel(\"x1\", fontsize = f_s)\n",
    "        ax.set_ylabel(\"x2\", fontsize = f_s)\n",
    "\n",
    "        fig.suptitle(\"Learned Decision Boundary\", fontsize = f_s)\n",
    "\n",
    "        plt.subplots_adjust(top = 0.90)\n",
    "      \n",
    "        logger = self.logger.experiment\n",
    "        logger.add_figure(name,  plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7370ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# Calculate: Basic Statistics\n",
    "#----------------------------\n",
    "\n",
    "def calculate_statistics(matrix):\n",
    "\n",
    "    results = {}\n",
    "    all_precision, all_recall, all_fscore, all_accuracy = [], [], [], []\n",
    "\n",
    "    for target_class in range(matrix.shape[0]):\n",
    "\n",
    "        true_positive = matrix[target_class, target_class]\n",
    "        false_negatives = np.sum(matrix[target_class, :] ) - true_positive\n",
    "        false_positives = np.sum(matrix[:, target_class] ) - true_positive\n",
    "\n",
    "        if(true_positive != 0):            \n",
    "            precision = true_positive / ( true_positive + false_positives ) \n",
    "            recall = true_positive / ( true_positive + false_negatives ) \n",
    "            fscore = ( 2 * recall * precision ) / ( recall + precision )\n",
    "            accuracy = true_positive / np.sum(matrix[target_class, : ])\n",
    "        else:\n",
    "            precision = recall = fscore = accuracy = 0\n",
    "\n",
    "        all_precision.append(np.round(precision, 3))\n",
    "        all_accuracy.append(np.round(accuracy, 3))\n",
    "        all_fscore.append(np.round(fscore, 3))\n",
    "        all_recall.append(np.round(recall, 3))\n",
    "\n",
    "    results['model_precision'] = np.round(np.mean(all_precision), 3)\n",
    "    results['model_accuracy'] = np.round(np.mean(all_accuracy), 3)\n",
    "    results['model_recall'] = np.round(np.mean(all_recall), 3)\n",
    "    results['model_fscore'] = np.round(np.mean(all_fscore), 3)\n",
    "\n",
    "    results['class_precision'] = all_precision\n",
    "    results['class_accuracy'] = all_accuracy\n",
    "    results['class_recall'] = all_recall\n",
    "    results['class_fscore'] = all_fscore \n",
    "\n",
    "    results['confusion'] = matrix\n",
    "\n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-melbourne",
   "metadata": {},
   "source": [
    "## <ins> Pytorch Lightning Trainer </ins>\n",
    "\n",
    "Lastly, we just need a **Pytorch Lightning Trainer** that will take the **Pytorch Dataset and Model** and begin a training process. We also put everything together that we initialized above in the code section below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48620c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize: Gloabl Seed\n",
    "\n",
    "seed_everything(seed, workers = True)\n",
    "\n",
    "# Generate: Synthetic Dataset\n",
    "     \n",
    "dataset = load_data(path_dataset)\n",
    "        \n",
    "params[\"train\"] = dataset\n",
    "    \n",
    "# Initialize: Formatter\n",
    "\n",
    "dataset = PLDM(paramsa)\n",
    "\n",
    "# Initialize: Model\n",
    "\n",
    "model = MLP(params)\n",
    "\n",
    "# Initialize: Logger \n",
    "\n",
    "if(logger_choice == 0):\n",
    "    logger = pl_loggers.TensorBoardLogger(path_results, name = \"\", version = 0)\n",
    "else:\n",
    "    logger = Logger(path_results, name = \"\", version = 0)\n",
    "\n",
    "# Train: Model\n",
    "\n",
    "if(use_gpu):\n",
    "\n",
    "    # Initialize: GPU Trainer\n",
    "\n",
    "    trainer = Trainer( logger = logger,\n",
    "                       deterministic = True,\n",
    "                       default_root_dir = path_results,\n",
    "                       check_val_every_n_epoch = valid_rate,\n",
    "                       max_epochs = num_epochs, num_nodes = 1,\n",
    "                       num_sanity_val_steps = 0, gpus = gpu_list,\n",
    "                       plugins = DDPPlugin(find_unused_parameters=False, ) )\n",
    "else:\n",
    "\n",
    "    # Initialize: CPU Trainer\n",
    "\n",
    "    trainer = Trainer( logger = logger,\n",
    "                       deterministic = True,\n",
    "                       max_epochs = num_epochs,\n",
    "                       num_sanity_val_steps = 0,\n",
    "                       default_root_dir = path_results,\n",
    "                       check_val_every_n_epoch = valid_rate )\n",
    "\n",
    "trainer.fit(model, dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
