{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cba3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: MINDFUL\n",
    "# Purpose: Configuration ( Linear Regression )\n",
    "\n",
    "#--------------------------------\n",
    "# Parameters: All Paths (I / O)\n",
    "#--------------------------------\n",
    "\n",
    "path_results = \"/develop/results/linear_regression/\"\n",
    "path_dataset = \"/develop/data/regression/linear/data.csv\"\n",
    "\n",
    "#-------------------------------\n",
    "# Parameters: Training Model\n",
    "#-------------------------------\n",
    "\n",
    "# Config: Validation Rate\n",
    "\n",
    "valid_rate = 1\n",
    "\n",
    "# Config: Randomization\n",
    "\n",
    "seed = 123 \n",
    "\n",
    "# Config: CPU \n",
    "\n",
    "num_workers = 1\n",
    "\n",
    "# Config: GPU\n",
    "\n",
    "use_gpu = 0\n",
    "gpu_list = [0, 1]\n",
    "\n",
    "# Config: Gradient Descent\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 100 \n",
    "learning_rate = 0.01\n",
    "\n",
    "# Create: Parameter Container \n",
    "\n",
    "params = { \"path_results\": path_results, \"path_dataset\": path_dataset,\n",
    "           \"valid_rate\": valid_rate, \"seed\": seed, \"num_workers\": num_workers, \"use_gpu\": use_gpu, \n",
    "           \"gpu_list\": gpu_list, \"batch_size\": batch_size, \"num_epochs\": num_epochs, \"learning_rate\": learning_rate }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cadceed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Before we begin, lets import required libraries (Basic & Custom). Below is a summary their importance:\n",
    "\n",
    "#--------------------------------\n",
    "# Import: Basic Python Libraries\n",
    "#--------------------------------\n",
    "\n",
    "# 1) os: system operations (e.g., reading and writing files/folders) \n",
    "# 2) cv2: Basic image operations (e.g. reading images), image processing, computer vision, and more\n",
    "# 3) shutil: system operations similar to os but with some extra utility\n",
    "# 4) Numpy: Linear algebra, data representation (e.g., matrices, vectors), and more\n",
    "# 5) Matplotlib: Visualizations / Plots\n",
    "# 6) Torch: Why you are here today (pytorch) \n",
    "# 7) Pytorch Lightning: Pytorch but more organized and simplified. This is likely the future of pytorch.\n",
    "\n",
    "#--------------------------------\n",
    "# Import: Custom Python Libraries\n",
    "#--------------------------------\n",
    "\n",
    "# 1) Logger: Experiment logging tool for results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba7c53f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'custom_logger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52/3152703774.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#--------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcustom_logger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'custom_logger'"
     ]
    }
   ],
   "source": [
    "#--------------------------------\n",
    "# Import: Basic Python Libraries\n",
    "#--------------------------------\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Optional\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning import LightningModule, LightningDataModule\n",
    "\n",
    "#--------------------------------\n",
    "# Import: Custom Python Libraries\n",
    "#--------------------------------\n",
    "\n",
    "from custom_logger import Logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4534a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets prepare our dataset loader. Its just a CSV file so we can load it appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68d7dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# Initialize: Custom Dataset \n",
    "#--------------------------------\n",
    "\n",
    "class Dataset:\n",
    "\n",
    "    def __init__(self, samples, labels):\n",
    "\n",
    "        self.labels = labels\n",
    "        self.samples = samples\n",
    "\n",
    "#--------------------------------\n",
    "# Load: Training Dataset (.CSV)\n",
    "#--------------------------------\n",
    "\n",
    "def load_data(path):\n",
    "\n",
    "    data_file = open(path, \"r\")\n",
    "\n",
    "    data = []\n",
    "    for line in data_file:\n",
    "        data.append([ float(ele.strip(\"\\n\")) for ele in line.split(\",\") ])\n",
    "\n",
    "    data = np.asarray(data)\n",
    "\n",
    "    samples, labels = data[:, :-1], data[:, -1]\n",
    "\n",
    "    return Dataset(samples, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0b27496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pytorch, datasets need a specific format in order to take advantage of pytorch utitlies (e.g., dataloaders).\n",
    "# Because of this, lets make a data module that accomplishes this requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a96e1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLDM(LightningDataModule):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super().__init__() \n",
    "                           \n",
    "        # Load: Dataset Parameters\n",
    "                           \n",
    "        self.data = params[\"train\"]\n",
    "                           \n",
    "        # Load: Processing Parameters\n",
    "\n",
    "        self.batch = params[\"batch_size\"]\n",
    "        self.workers = params[\"num_workers\"]\n",
    "        \n",
    "    #----------------------------\n",
    "    # Create: Training Datasets \n",
    "    #----------------------------\n",
    "                           \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "\n",
    "        # Create: Pytorch Datasets\n",
    "\n",
    "        self.train = Dataset(self.data)\n",
    "        self.valid = Dataset(self.data)\n",
    "\n",
    "    #----------------------------\n",
    "    # Create: Training DataLoader\n",
    "    #----------------------------\n",
    "\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        return DataLoader( self.train, batch_size = self.batch,\n",
    "                           num_workers = self.workers, shuffle = 1, persistent_workers = 1 )\n",
    "\n",
    "    #----------------------------\n",
    "    # Create: Validation Loader\n",
    "    #----------------------------\n",
    "\n",
    "    def val_dataloader(self):\n",
    "\n",
    "        return DataLoader( self.valid, batch_size = self.batch,\n",
    "                           num_workers = self.workers, persistent_workers = 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "444b9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# Initialize: Lightining Model\n",
    "#--------------------------------\n",
    "\n",
    "class Linear_Regression(LightningModule):\n",
    "\n",
    "    def __init__(self, params):\n",
    "    \n",
    "        super().__init__()\n",
    "        \n",
    "        # Load: Model Parameters\n",
    "        \n",
    "        self.max_epochs = params[\"num_epochs\"]\n",
    "        self.learning_rate = params[\"learning_rate\"]\n",
    "        \n",
    "        # Initialize: Regression Model \n",
    "        \n",
    "        self.regressor = torch.nn.Linear(1, 1)\n",
    "        \n",
    "    #----------------------------      \n",
    "    # Create: Objective Function       \n",
    "    #----------------------------      \n",
    "                                       \n",
    "    def objective(self, preds, labels):\n",
    "    \n",
    "        # Format: Labels\n",
    "    \n",
    "        labels = labels.type(preds.type())\n",
    "    \n",
    "        # Objective: Mean Squared Error\n",
    "        \n",
    "        cost = nn.MSELoss()\n",
    "        \n",
    "    #----------------------------\n",
    "    # Create: Optimizer Function\n",
    "    #----------------------------\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    #----------------------------\n",
    "    # Create: Model Forward Pass\n",
    "    #----------------------------\n",
    "\n",
    "    def forward(self, samples):\n",
    "\n",
    "        return self.regressor(samples)\n",
    "\n",
    "    #----------------------------\n",
    "    # Create: Train Cycle (Epoch)\n",
    "    #----------------------------\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # Load: Data Batch\n",
    "\n",
    "        samples, labels = batch\n",
    "\n",
    "        preds = self(samples)\n",
    "\n",
    "        # Calculate: Training Loss\n",
    "   \n",
    "        loss = self.objective(preds, labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    #----------------------------\n",
    "    # Run: Post Training Script\n",
    "    #----------------------------\n",
    "\n",
    "    def training_epoch_end(self, train_step_outputs):\n",
    "\n",
    "        # Update: Training Plots\n",
    "\n",
    "        if(self.current_epoch > 0):\n",
    "\n",
    "            logger = self.logger.experiment\n",
    "\n",
    "            logger.log_training_loss(self.current_epoch)\n",
    "\n",
    "            # Finalize: Learned Features & Metrics ( Video )\n",
    "\n",
    "            if(self.current_epoch == self.max_epochs - 1):\n",
    "\n",
    "                logger.finalize()\n",
    "\n",
    "    #----------------------------\n",
    "    # Create: Validation Cycle \n",
    "    #----------------------------\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        samples, labels = batch\n",
    "\n",
    "        preds = self(samples)\n",
    "\n",
    "        return samples, labels, preds\n",
    "\n",
    "    #----------------------------\n",
    "    # Run: Post Validation Script\n",
    "    #----------------------------\n",
    "\n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "\n",
    "        # Organize: Validation Outputs\n",
    "\n",
    "        all_samples, all_labels, all_preds = [], [], []\n",
    "\n",
    "        for group in val_step_outputs:\n",
    "\n",
    "            samples, labels, preds = group\n",
    "\n",
    "            all_labels.append( labels )\n",
    "            all_samples.append( samples )\n",
    "            all_preds.append( preds.detach() )\n",
    "\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_samples = torch.cat(all_samples)\n",
    "\n",
    "        # Logger: Visualizations  \n",
    "\n",
    "        logger = self.logger.experiment\n",
    "        logger.log_linear_regression(all_samples, all_labels, all_preds, self.current_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29ecab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, lets create a \"Trainer\" that will train and validate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ded230cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52/2772364219.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Initialize: Logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Train: Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Logger' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize: Gloabl Seed\n",
    "\n",
    "seed_everything(seed, workers = True)\n",
    "\n",
    "# Generate: Synthetic Dataset\n",
    "     \n",
    "dataset = load_data(path_dataset)\n",
    "        \n",
    "params[\"train\"] = dataset\n",
    "    \n",
    "# Initialize: Formatter\n",
    "\n",
    "dataset = PLDM(params)\n",
    "\n",
    "# Initialize: Model\n",
    "\n",
    "model = Linear_Regression(params)\n",
    "\n",
    "# Initialize: Logger \n",
    "\n",
    "logger = Logger(path_results, name = \"\", version = 0)\n",
    "\n",
    "# Train: Model\n",
    "\n",
    "if(use_gpu):\n",
    "\n",
    "    # Initialize: GPU Trainer\n",
    "\n",
    "    trainer = Trainer( logger = logger,\n",
    "                       deterministic = True,\n",
    "                       default_root_dir = path_results,\n",
    "                       check_val_every_n_epoch = valid_rate,\n",
    "                       max_epochs = num_epochs, num_nodes = 1,\n",
    "                       num_sanity_val_steps = 0, gpus = gpu_list,\n",
    "                       plugins = DDPPlugin(find_unused_parameters=False, ) )\n",
    "else:\n",
    "\n",
    "    # Initialize: CPU Trainer\n",
    "\n",
    "    trainer = Trainer( logger = logger,\n",
    "                       deterministic = True,\n",
    "                       max_epochs = num_epochs,\n",
    "                       num_sanity_val_steps = 0,\n",
    "                       default_root_dir = path_results,\n",
    "                       check_val_every_n_epoch = valid_rate )\n",
    "\n",
    "trainer.fit(model, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b57deca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
